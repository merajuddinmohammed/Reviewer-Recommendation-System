"""
Train LambdaRank Model for Reviewer Recommendation

This script trains a LightGBM LambdaRank model using the synthetic training data
generated by build_training_data.py.

LambdaRank is a learning-to-rank algorithm that optimizes ranking metrics like NDCG
(Normalized Discounted Cumulative Gain) directly. It's particularly well-suited for
recommendation systems where the goal is to rank candidates by relevance.

Model Architecture:
- Algorithm: LightGBM with LambdaRank objective
- Objective: lambdarank (pairwise ranking)
- Metric: NDCG@5, NDCG@10
- Features: 9 features from ranker (TF-IDF, embeddings, recency, pub_count, COI)
- Grouping: One group per query (critical for LTR)

Training Process:
1. Load training data from parquet
2. Split by query groups (train/valid)
3. Train LightGBM with early stopping
4. Evaluate on validation set (NDCG@5, NDCG@10)
5. Save model in two formats:
   - .txt (human-readable)
   - .pkl (for inference)
6. Print feature importances

CLI Arguments:
- --data: Path to training parquet file
- --out-txt: Output path for text model
- --out-pkl: Output path for pickle model
- --test-size: Validation split ratio (default: 0.2)
- --seed: Random seed for reproducibility
- --early-stopping-rounds: Early stopping patience (default: 50)
- --num-leaves: Max leaves in tree (default: 31)
- --learning-rate: Learning rate (default: 0.05)
- --n-estimators: Number of boosting rounds (default: 500)
- --min-data-in-leaf: Min samples per leaf (default: 20)

Author: Applied AI Assignment
Date: December 2024
"""

import logging
import argparse
import sys
import pickle
from pathlib import Path
from typing import Tuple, Dict, List

import numpy as np
import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import GroupShuffleSplit

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================================
# Data Loading and Splitting
# ============================================================================

def load_training_data(data_path: Path) -> pd.DataFrame:
    """
    Load training data from parquet file.
    
    Args:
        data_path: Path to training parquet file
        
    Returns:
        DataFrame with columns: query_id, author_id, y, group, features...
    """
    logger.info(f"Loading training data from {data_path}...")
    
    try:
        df = pd.read_parquet(data_path)
        
        logger.info(f"  Loaded {len(df)} samples")
        logger.info(f"  Queries: {df['query_id'].nunique()}")
        logger.info(f"  Groups: {df['group'].nunique()}")
        logger.info(f"  Positive samples: {df['y'].sum()} ({df['y'].mean():.2%})")
        logger.info(f"  Negative samples: {(df['y'] == 0).sum()}")
        
        return df
        
    except Exception as e:
        logger.error(f"Failed to load training data: {e}")
        raise


def split_by_query_groups(
    df: pd.DataFrame,
    test_size: float = 0.2,
    seed: int = 42
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Split data by query groups (not individual samples).
    
    Ensures that all samples from the same query are in either train or valid,
    but not split between them. This is critical for learning-to-rank evaluation.
    
    Args:
        df: DataFrame with 'group' column
        test_size: Fraction of groups for validation
        seed: Random seed
        
    Returns:
        Tuple of (train_df, valid_df)
    """
    logger.info(f"Splitting data by query groups...")
    logger.info(f"  Test size: {test_size:.1%}")
    logger.info(f"  Random seed: {seed}")
    
    # Get unique groups
    groups = df['group'].values
    unique_groups = np.unique(groups)
    
    # Split groups
    gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)
    train_idx, valid_idx = next(gss.split(df, groups=groups))
    
    train_df = df.iloc[train_idx].reset_index(drop=True)
    valid_df = df.iloc[valid_idx].reset_index(drop=True)
    
    logger.info(f"  Train: {len(train_df)} samples, {train_df['query_id'].nunique()} queries")
    logger.info(f"  Valid: {len(valid_df)} samples, {valid_df['query_id'].nunique()} queries")
    logger.info(f"  Train positives: {train_df['y'].sum()} ({train_df['y'].mean():.2%})")
    logger.info(f"  Valid positives: {valid_df['y'].sum()} ({valid_df['y'].mean():.2%})")
    
    return train_df, valid_df


def prepare_lgb_dataset(
    df: pd.DataFrame,
    feature_cols: List[str],
    label_col: str = 'y',
    group_col: str = 'group'
) -> lgb.Dataset:
    """
    Prepare LightGBM dataset with grouping information.
    
    Args:
        df: DataFrame with features, labels, and groups
        feature_cols: List of feature column names
        label_col: Name of label column
        group_col: Name of group column
        
    Returns:
        LightGBM Dataset with group information
    """
    X = df[feature_cols].values
    y = df[label_col].values
    
    # Compute group sizes (number of samples per group)
    # LightGBM expects an array of group sizes, not group IDs
    group_sizes = df.groupby(group_col).size().values
    
    logger.debug(f"  Features shape: {X.shape}")
    logger.debug(f"  Labels shape: {y.shape}")
    logger.debug(f"  Number of groups: {len(group_sizes)}")
    logger.debug(f"  Group sizes: min={group_sizes.min()}, max={group_sizes.max()}, mean={group_sizes.mean():.1f}")
    
    # Create dataset
    dataset = lgb.Dataset(
        X,
        label=y,
        group=group_sizes,
        feature_name=feature_cols,
        free_raw_data=False
    )
    
    return dataset


# ============================================================================
# Model Training
# ============================================================================

def train_lambdarank_model(
    train_df: pd.DataFrame,
    valid_df: pd.DataFrame,
    feature_cols: List[str],
    params: Dict,
    num_boost_round: int = 500,
    early_stopping_rounds: int = 50,
    verbose_eval: int = 50
) -> lgb.Booster:
    """
    Train LightGBM LambdaRank model.
    
    Args:
        train_df: Training DataFrame
        valid_df: Validation DataFrame
        feature_cols: List of feature column names
        params: LightGBM parameters
        num_boost_round: Number of boosting rounds
        early_stopping_rounds: Early stopping patience
        verbose_eval: Print metrics every N rounds
        
    Returns:
        Trained LightGBM Booster
    """
    logger.info("=" * 80)
    logger.info("Training LightGBM LambdaRank Model")
    logger.info("=" * 80)
    
    # Prepare datasets
    logger.info("Preparing datasets...")
    train_data = prepare_lgb_dataset(train_df, feature_cols)
    valid_data = prepare_lgb_dataset(valid_df, feature_cols)
    
    logger.info("")
    logger.info("Model Parameters:")
    for key, value in params.items():
        logger.info(f"  {key}: {value}")
    logger.info(f"  num_boost_round: {num_boost_round}")
    logger.info(f"  early_stopping_rounds: {early_stopping_rounds}")
    logger.info("")
    
    # Train model
    logger.info("Training model...")
    logger.info("")
    
    callbacks = [
        lgb.log_evaluation(period=verbose_eval),
        lgb.early_stopping(stopping_rounds=early_stopping_rounds, verbose=True)
    ]
    
    model = lgb.train(
        params,
        train_data,
        num_boost_round=num_boost_round,
        valid_sets=[train_data, valid_data],
        valid_names=['train', 'valid'],
        callbacks=callbacks
    )
    
    logger.info("")
    logger.info(f"Training complete!")
    logger.info(f"  Best iteration: {model.best_iteration}")
    logger.info(f"  Best score: {model.best_score}")
    
    return model


# ============================================================================
# Model Evaluation
# ============================================================================

def evaluate_model(
    model: lgb.Booster,
    df: pd.DataFrame,
    feature_cols: List[str],
    dataset_name: str = "validation"
) -> Dict[str, float]:
    """
    Evaluate model on a dataset.
    
    Computes NDCG@5 and NDCG@10 by making predictions and computing manually.
    
    Args:
        model: Trained LightGBM model
        df: DataFrame to evaluate on
        feature_cols: Feature column names
        dataset_name: Name for logging
        
    Returns:
        Dictionary with NDCG scores
    """
    logger.info(f"Evaluating model on {dataset_name} set...")
    
    # Make predictions
    X = df[feature_cols].values
    y_pred = model.predict(X)
    
    # Get from model's best_score if available
    results = {}
    
    # Extract metrics from model's validation results if this is validation set
    if dataset_name == 'valid' and hasattr(model, 'best_score'):
        if 'valid' in model.best_score:
            valid_scores = model.best_score['valid']
            if 'ndcg@5' in valid_scores:
                results['ndcg@5'] = valid_scores['ndcg@5']
                logger.info(f"  NDCG@5: {results['ndcg@5']:.4f}")
            if 'ndcg@10' in valid_scores:
                results['ndcg@10'] = valid_scores['ndcg@10']
                logger.info(f"  NDCG@10: {results['ndcg@10']:.4f}")
    elif dataset_name == 'train' and hasattr(model, 'best_score'):
        if 'train' in model.best_score:
            train_scores = model.best_score['train']
            if 'ndcg@5' in train_scores:
                results['ndcg@5'] = train_scores['ndcg@5']
                logger.info(f"  NDCG@5: {results['ndcg@5']:.4f}")
            if 'ndcg@10' in train_scores:
                results['ndcg@10'] = train_scores['ndcg@10']
                logger.info(f"  NDCG@10: {results['ndcg@10']:.4f}")
    
    # If we couldn't get scores from best_score, set to 0
    if not results:
        results = {'ndcg@5': 0.0, 'ndcg@10': 0.0}
        logger.warning(f"  Could not compute NDCG scores")
    
    return results


def print_feature_importances(
    model: lgb.Booster,
    feature_cols: List[str],
    importance_type: str = 'gain'
) -> pd.DataFrame:
    """
    Print and return feature importances.
    
    Args:
        model: Trained LightGBM model
        feature_cols: Feature column names
        importance_type: Type of importance ('gain', 'split', or 'weight')
        
    Returns:
        DataFrame with feature importances
    """
    logger.info("")
    logger.info(f"Feature Importances (importance_type={importance_type}):")
    logger.info("-" * 60)
    
    importances = model.feature_importance(importance_type=importance_type)
    
    # Create DataFrame
    importance_df = pd.DataFrame({
        'feature': feature_cols,
        'importance': importances
    }).sort_values('importance', ascending=False)
    
    # Normalize to percentage
    importance_df['importance_pct'] = 100 * importance_df['importance'] / importance_df['importance'].sum()
    
    # Print
    for idx, row in importance_df.iterrows():
        logger.info(f"  {row['feature']:<20} {row['importance']:>10.1f} ({row['importance_pct']:>5.1f}%)")
    
    logger.info("-" * 60)
    logger.info(f"  Total: {importance_df['importance'].sum():.1f}")
    
    return importance_df


# ============================================================================
# Model Saving
# ============================================================================

def save_model(
    model: lgb.Booster,
    txt_path: Path,
    pkl_path: Path
) -> None:
    """
    Save model in two formats.
    
    Args:
        model: Trained LightGBM model
        txt_path: Path to save text model
        pkl_path: Path to save pickle model
    """
    logger.info("")
    logger.info("Saving model...")
    
    # Create directories if needed
    txt_path.parent.mkdir(parents=True, exist_ok=True)
    pkl_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Save as text (human-readable, LightGBM native format)
    logger.info(f"  Saving text model to {txt_path}...")
    model.save_model(str(txt_path))
    logger.info(f"  ✓ Text model saved ({txt_path.stat().st_size / 1024:.1f} KB)")
    
    # Save as pickle (for Python inference)
    logger.info(f"  Saving pickle model to {pkl_path}...")
    with open(pkl_path, 'wb') as f:
        pickle.dump(model, f)
    logger.info(f"  ✓ Pickle model saved ({pkl_path.stat().st_size / 1024:.1f} KB)")


# ============================================================================
# Main Training Pipeline
# ============================================================================

def train_ranker(
    data_path: Path,
    txt_path: Path,
    pkl_path: Path,
    test_size: float = 0.2,
    seed: int = 42,
    early_stopping_rounds: int = 50,
    num_leaves: int = 31,
    learning_rate: float = 0.05,
    n_estimators: int = 500,
    min_data_in_leaf: int = 20
) -> bool:
    """
    Main training pipeline.
    
    Args:
        data_path: Path to training parquet file
        txt_path: Output path for text model
        pkl_path: Output path for pickle model
        test_size: Validation split ratio
        seed: Random seed
        early_stopping_rounds: Early stopping patience
        num_leaves: Max leaves in tree
        learning_rate: Learning rate
        n_estimators: Number of boosting rounds
        min_data_in_leaf: Min samples per leaf
        
    Returns:
        True if successful, False otherwise
    """
    logger.info("=" * 80)
    logger.info("LambdaRank Model Training Pipeline")
    logger.info("=" * 80)
    logger.info("")
    
    # -------------------------------------------------------------------------
    # Step 1: Load data
    # -------------------------------------------------------------------------
    logger.info("Step 1: Loading training data...")
    
    try:
        df = load_training_data(data_path)
    except Exception as e:
        logger.error(f"Failed to load data: {e}")
        return False
    
    # Define feature columns
    feature_cols = [
        'tfidf_max', 'tfidf_mean',
        'emb_max', 'emb_mean',
        'topic_overlap',
        'recency_mean', 'recency_max',
        'pub_count', 'coi_flag'
    ]
    
    # Verify columns exist
    missing_cols = set(feature_cols) - set(df.columns)
    if missing_cols:
        logger.error(f"Missing columns: {missing_cols}")
        return False
    
    logger.info(f"  Features: {len(feature_cols)} columns")
    logger.info(f"  Features: {', '.join(feature_cols)}")
    
    # -------------------------------------------------------------------------
    # Step 2: Split data
    # -------------------------------------------------------------------------
    logger.info("")
    logger.info("Step 2: Splitting data by query groups...")
    
    try:
        train_df, valid_df = split_by_query_groups(df, test_size=test_size, seed=seed)
    except Exception as e:
        logger.error(f"Failed to split data: {e}")
        return False
    
    # -------------------------------------------------------------------------
    # Step 3: Train model
    # -------------------------------------------------------------------------
    logger.info("")
    logger.info("Step 3: Training LambdaRank model...")
    
    # Configure parameters (optimized for imbalanced ranking data)
    params = {
        'objective': 'lambdarank',
        'metric': 'ndcg',
        'ndcg_eval_at': [5, 10],
        'num_leaves': num_leaves,
        'learning_rate': learning_rate,
        'min_data_in_leaf': min_data_in_leaf,
        'max_depth': 6,  # Limit depth to prevent overfitting
        'min_gain_to_split': 0.1,  # Require meaningful splits
        'feature_fraction': 0.9,  # Use more features (we only have 9)
        'bagging_fraction': 0.9,  # Use more data
        'bagging_freq': 3,  # More frequent bagging
        'lambda_l1': 0.1,  # L1 regularization
        'lambda_l2': 0.1,  # L2 regularization
        'min_data_per_group': 3,  # Minimum samples per query group
        'max_bin': 255,  # Default bin size
        'verbose': -1,
        'seed': seed,
        'force_col_wise': True,  # Faster for small datasets
        'num_threads': 0  # Use all CPU cores
    }
    
    try:
        model = train_lambdarank_model(
            train_df=train_df,
            valid_df=valid_df,
            feature_cols=feature_cols,
            params=params,
            num_boost_round=n_estimators,
            early_stopping_rounds=early_stopping_rounds,
            verbose_eval=50
        )
    except Exception as e:
        logger.error(f"Failed to train model: {e}")
        return False
    
    # -------------------------------------------------------------------------
    # Step 4: Evaluate model
    # -------------------------------------------------------------------------
    logger.info("")
    logger.info("Step 4: Evaluating model...")
    logger.info("")
    
    try:
        train_metrics = evaluate_model(model, train_df, feature_cols, "train")
        valid_metrics = evaluate_model(model, valid_df, feature_cols, "valid")
    except Exception as e:
        logger.error(f"Failed to evaluate model: {e}")
        return False
    
    # -------------------------------------------------------------------------
    # Step 5: Feature importances
    # -------------------------------------------------------------------------
    try:
        importance_df = print_feature_importances(model, feature_cols, importance_type='gain')
    except Exception as e:
        logger.error(f"Failed to compute feature importances: {e}")
        return False
    
    # -------------------------------------------------------------------------
    # Step 6: Save model
    # -------------------------------------------------------------------------
    try:
        save_model(model, txt_path, pkl_path)
    except Exception as e:
        logger.error(f"Failed to save model: {e}")
        return False
    
    # -------------------------------------------------------------------------
    # Summary
    # -------------------------------------------------------------------------
    logger.info("")
    logger.info("=" * 80)
    logger.info("Training Complete!")
    logger.info("=" * 80)
    logger.info("")
    logger.info("Final Metrics:")
    logger.info(f"  Train NDCG@5:  {train_metrics['ndcg@5']:.4f}")
    logger.info(f"  Train NDCG@10: {train_metrics['ndcg@10']:.4f}")
    logger.info(f"  Valid NDCG@5:  {valid_metrics['ndcg@5']:.4f}")
    logger.info(f"  Valid NDCG@10: {valid_metrics['ndcg@10']:.4f}")
    logger.info("")
    logger.info("Model Files:")
    logger.info(f"  Text:   {txt_path}")
    logger.info(f"  Pickle: {pkl_path}")
    logger.info("")
    logger.info("Top 3 Features:")
    for idx, row in importance_df.head(3).iterrows():
        logger.info(f"  {idx + 1}. {row['feature']:<20} ({row['importance_pct']:.1f}%)")
    
    return True


# ============================================================================
# Main Entry Point
# ============================================================================

def main():
    """Main entry point for CLI."""
    parser = argparse.ArgumentParser(
        description="Train LightGBM LambdaRank model for reviewer recommendation",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Train with default parameters
  python train_ranker.py
  
  # Custom validation split and early stopping
  python train_ranker.py --test-size 0.3 --early-stopping-rounds 100
  
  # Adjust model complexity
  python train_ranker.py --num-leaves 63 --learning-rate 0.03 --n-estimators 1000
  
  # Custom output paths
  python train_ranker.py --out-txt models/ranker_v2.txt --out-pkl models/ranker_v2.pkl
        """
    )
    
    # Data arguments
    parser.add_argument(
        '--data',
        type=str,
        default='data/train.parquet',
        help='Path to training parquet file (default: data/train.parquet)'
    )
    
    parser.add_argument(
        '--out-txt',
        type=str,
        default='models/lgbm_ranker.txt',
        help='Output path for text model (default: models/lgbm_ranker.txt)'
    )
    
    parser.add_argument(
        '--out-pkl',
        type=str,
        default='models/lgbm_ranker.pkl',
        help='Output path for pickle model (default: models/lgbm_ranker.pkl)'
    )
    
    # Training arguments
    parser.add_argument(
        '--test-size',
        type=float,
        default=0.2,
        help='Validation split ratio (default: 0.2)'
    )
    
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='Random seed for reproducibility (default: 42)'
    )
    
    parser.add_argument(
        '--early-stopping-rounds',
        type=int,
        default=50,
        help='Early stopping patience (default: 50)'
    )
    
    # Model hyperparameters
    parser.add_argument(
        '--num-leaves',
        type=int,
        default=15,
        help='Max number of leaves in tree (default: 15)'
    )
    
    parser.add_argument(
        '--learning-rate',
        type=float,
        default=0.01,
        help='Learning rate (default: 0.01)'
    )
    
    parser.add_argument(
        '--n-estimators',
        type=int,
        default=1000,
        help='Number of boosting rounds (default: 1000)'
    )
    
    parser.add_argument(
        '--min-data-in-leaf',
        type=int,
        default=5,
        help='Minimum samples per leaf (default: 5)'
    )
    
    args = parser.parse_args()
    
    # Convert paths
    data_path = Path(args.data)
    txt_path = Path(args.out_txt)
    pkl_path = Path(args.out_pkl)
    
    # Validate inputs
    if not data_path.exists():
        logger.error(f"Training data not found: {data_path}")
        logger.error("Run build_training_data.py first to create training data")
        sys.exit(1)
    
    # Train model
    success = train_ranker(
        data_path=data_path,
        txt_path=txt_path,
        pkl_path=pkl_path,
        test_size=args.test_size,
        seed=args.seed,
        early_stopping_rounds=args.early_stopping_rounds,
        num_leaves=args.num_leaves,
        learning_rate=args.learning_rate,
        n_estimators=args.n_estimators,
        min_data_in_leaf=args.min_data_in_leaf
    )
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
